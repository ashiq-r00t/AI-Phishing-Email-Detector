{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9741df72-98ce-468a-a356-26e416eab190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nigerian_Fraud.csv', 'Ling.csv', 'dataset_exploration.ipynb', 'Nazario.csv', 'SpamAssasin.csv', '.ipynb_checkpoints', 'CEAS_08.csv', 'phishing_email.csv', 'Enron.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = '/Users/ashiq/Desktop/email dataset/'\n",
    "\n",
    "files = os.listdir(dataset_path)\n",
    "print(files[:10])  # list first 10 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd577428-c862-4c38-977f-ae8bea66adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sender,receiver,date,subject,body,urls,label\n",
      "MR. JAMES NGOLA. <james_ngola2002@maktoob.com>,webmaster@aclweb.org,\"Thu, 31 Oct 2002 02:38:20 +0000\",URGENT BUSINESS ASSISTANCE AND PARTNERSHIP,\"FROM:MR. JAMES NGOLA.\n",
      "CONFIDENTIAL TEL: 233-27-587908.\n",
      "E-MAIL: (james_ngola2002@maktoob.com).\n",
      "\n",
      "URGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\n",
      "\n",
      "\n",
      "DEAR FRIEND,\n",
      "\n",
      "I AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY GUARD ON 16TH JAN. 2001\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(dataset_path, files[0])\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content[:500])  # print first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762c8526-1adc-4b48-8d34-f358c245ffbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m dataset_path = \u001b[33m'\u001b[39m\u001b[33m/Users/ashiq/Desktop/email dataset/\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Replace with your actual username\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = '/Users/ashiq/Desktop/email dataset/'  # Replace with your actual username\n",
    "file_name = 'phishing_email.csv'\n",
    "\n",
    "file_path = os.path.join(dataset_path, file_name)\n",
    "\n",
    "# Load CSV file into a dataframe\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check summary information\n",
    "print(df.info())\n",
    "\n",
    "# Check the distribution of labels (replace 'label' with actual label column name)\n",
    "if 'label' in df.columns:\n",
    "    print(df['label'].value_counts())\n",
    "else:\n",
    "    print(\"No 'label' column in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ce6039-ea12-4070-92ab-2f0bb06cb01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.2 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c0f787-8631-40a3-a9ac-4c7b34a2a9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       text_combined  label\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...      0\n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...      0\n",
      "2  enron actuals march 30 april 1 201 estimated a...      0\n",
      "3  hpl nom may 30 2001 see attached file hplno 53...      0\n",
      "4  hpl nom june 1 2001 see attached file hplno 60...      0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82486 entries, 0 to 82485\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   text_combined  82486 non-null  object\n",
      " 1   label          82486 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "label\n",
      "1    42891\n",
      "0    39595\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = '/Users/ashiq/Desktop/email dataset/'  # Replace with your actual username\n",
    "file_name = 'phishing_email.csv'\n",
    "\n",
    "file_path = os.path.join(dataset_path, file_name)\n",
    "\n",
    "# Load CSV file into a dataframe\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check summary information\n",
    "print(df.info())\n",
    "\n",
    "# Check the distribution of labels (replace 'label' with actual label column name)\n",
    "if 'label' in df.columns:\n",
    "    print(df['label'].value_counts())\n",
    "else:\n",
    "    print(\"No 'label' column in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34335804-a3cb-4d33-9f5c-f10eb73eb99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpl nom may 25 2001 see attached file hplno 525 xls hplno 525 xls\n",
      "nom actual vols 24 th forwarded sabrae zajac hou ect 05 30 2001 12 07 pm enron capital trade resources corp eileen ponton 05 29 2001 08 37 davilal txu com cstonel txu com mjones 7 txu com hpl scheduling enron com liz bellamy enron com szajac enron com cc subject nom actual vols 24 th agree nomination 33 750 forwarded eileen ponton houston pefs pec 05 29 01 08 36 charlie stone eileen ponton melissa jones com hpl scheduling enron com liz bellamy enron com szajac enron com 05 25 01 subject nom actual vols 24 th 04 23 pm agree nominated volume records reflect following nom schedule 30 rate eff 0900 hrs hour beginning 1400 hrs 6 250 60 rate eff 1400 hrs hour beginning 1700 hrs 7 500 30 rate eff 1700 hrs hour beginning 0900 hrs 20 000 total nominated 33 750 please review source data let us know agree thanks ccs eileen ponton 05 25 2001 04 06 50 pm david avila lsp enserch us tu charlie stone energy txu tu melissa jones energy txu tu hpl scheduling enron com liz bellamy enron com szajac enron com cc subject nom actual vols 24 th nom mcf mmbtu 27 500 33 109 34 003\n"
     ]
    }
   ],
   "source": [
    "print(df['text_combined'].iloc[0])  # First email text\n",
    "print(df['text_combined'].iloc[1])  # Second email text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b39a39-fcc2-4677-b1c5-b4c9a1d3c83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1 text:\n",
      "hpl nom may 25 2001 see attached file hplno 525 xls hplno 525 xls\n",
      "\n",
      "Email 2 text:\n",
      "nom actual vols 24 th forwarded sabrae zajac hou ect 05 30 2001 12 07 pm enron capital trade resources corp eileen ponton 05 29 2001 08 37 davilal txu com cstonel txu com mjones 7 txu com hpl scheduling enron com liz bellamy enron com szajac enron com cc subject nom actual vols 24 th agree nomination 33 750 forwarded eileen ponton houston pefs pec 05 29 01 08 36 charlie stone eileen ponton melissa jones com hpl scheduling enron com liz bellamy enron com szajac enron com 05 25 01 subject nom actual vols 24 th 04 23 pm agree nominated volume records reflect following nom schedule 30 rate eff 0900 hrs hour beginning 1400 hrs 6 250 60 rate eff 1400 hrs hour beginning 1700 hrs 7 500 30 rate eff 1700 hrs hour beginning 0900 hrs 20 000 total nominated 33 750 please review source data let us know agree thanks ccs eileen ponton 05 25 2001 04 06 50 pm david avila lsp enserch us tu charlie stone energy txu tu melissa jones energy txu tu hpl scheduling enron com liz bellamy enron com szajac enron com cc subject nom actual vols 24 th nom mcf mmbtu 27 500 33 109 34 003\n",
      "\n",
      "Email 3 text:\n",
      "enron actuals march 30 april 1 201 estimated actuals march 30 2001 flow march 31 2001 flow april 1 2001 teco tap 35 000 receive actuals duke forward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 3 email texts\n",
    "for i in range(3):\n",
    "    print(f\"Email {i+1} text:\\n{df['text_combined'].iloc[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d574e6-3405-4724-bc64-8f2883c4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers except spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01bc1b0f-6c40-4a75-bdc5-265cd4b1093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       text_combined  \\\n",
      "0  hpl nom may 25 2001 see attached file hplno 52...   \n",
      "1  nom actual vols 24 th forwarded sabrae zajac h...   \n",
      "2  enron actuals march 30 april 1 201 estimated a...   \n",
      "3  hpl nom may 30 2001 see attached file hplno 53...   \n",
      "4  hpl nom june 1 2001 see attached file hplno 60...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  hpl nom may see attached file hplno xls hplno xls  \n",
      "1  nom actual vols th forwarded sabrae zajac hou ...  \n",
      "2  enron actuals march april estimated actuals ma...  \n",
      "3  hpl nom may see attached file hplno xls hplno xls  \n",
      "4  hpl nom june see attached file hplno xls hplno...  \n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning to first 5 emails as a test\n",
    "df['cleaned_text'] = df['text_combined'].head(5).apply(clean_text)\n",
    "print(df[['text_combined', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e031dd6-6adf-4d72-91ac-cdafd0f6878f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use cleaned_text column for vectorization (use full dataset here)\u001b[39;00m\n\u001b[32m      4\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mtext_combined\u001b[39m\u001b[33m'\u001b[39m].apply(clean_text)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use cleaned_text column for vectorization (use full dataset here)\n",
    "df['cleaned_text'] = df['text_combined'].apply(clean_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the email texts into TF-IDF features\n",
    "X = tfidf.fit_transform(df['cleaned_text'])\n",
    "\n",
    "print(f\"Shape of TF-IDF matrix: {X.shape}\")  # (number of emails, number of features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919b5849-bff1-428b-9995-fbbb118e2d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "360f36a3-890d-44ad-9a42-0bd07ad049c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (82486, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use cleaned_text column for vectorization (use full dataset here)\n",
    "df['cleaned_text'] = df['text_combined'].apply(clean_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the email texts into TF-IDF features\n",
    "X = tfidf.fit_transform(df['cleaned_text'])\n",
    "\n",
    "print(f\"Shape of TF-IDF matrix: {X.shape}\")  # (number of emails, number of features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27183556-b002-46af-910b-add65a6c3d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 65988 samples\n",
      "Test set size: (16498, 5000) samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target labels\n",
    "y = df['label']\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c66d8f-e290-4e26-9300-58f24a5d7b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9791489877560916\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      7935\n",
      "           1       0.98      0.98      0.98      8563\n",
      "\n",
      "    accuracy                           0.98     16498\n",
      "   macro avg       0.98      0.98      0.98     16498\n",
      "weighted avg       0.98      0.98      0.98     16498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize and train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ead03d-369f-4d45-811d-a88b812342c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
